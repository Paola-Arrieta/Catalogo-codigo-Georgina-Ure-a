{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión exitosa a Redshift\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gurenab\\AppData\\Local\\Temp\\ipykernel_78508\\1390778327.py:30: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  query = pd.read_sql_query(\n"
     ]
    }
   ],
   "source": [
    "# Conexión a Redshift\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    " \n",
    "# conn var\n",
    "host = '172.17.139.149'\n",
    "port = 5439 \n",
    "database = 'datasets'\n",
    "user = 'gurenab'\n",
    "password = 'Redshift2023.'\n",
    " \n",
    "#create conn\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=database,\n",
    "        user=user,\n",
    "        password=password\n",
    "        )\n",
    "    print(\"Conexión exitosa a Redshift\")\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Error al conectar a Redshift: {err}\")\n",
    "\n",
    "#Consulta SQL\n",
    "query = \"SELECT * FROM datasets.aioperations.portfoliobystate_dst;\"\n",
    "query = pd.read_sql_query(\n",
    "                            \"\"\"\n",
    "                            SELECT\n",
    "                            *\n",
    "                            FROM datasets.aioperations.portfoliobystate_dst\n",
    "                            \"\"\",\n",
    "                            conn,\n",
    "                        )\n",
    " \n",
    " \n",
    "df = pd.DataFrame(query)\n",
    " \n",
    "# Cerrar la conexión\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4339065 entries, 0 to 4339064\n",
      "Data columns (total 29 columns):\n",
      " #   Column                     Dtype         \n",
      "---  ------                     -----         \n",
      " 0   cli_identification         object        \n",
      " 1   cli_original_code          object        \n",
      " 2   fullname                   object        \n",
      " 3   accounttype                object        \n",
      " 4   countryname                object        \n",
      " 5   institutiondescription     object        \n",
      " 6   alchemydescription         object        \n",
      " 7   operationnumber            object        \n",
      " 8   include_date               datetime64[ns]\n",
      " 9   statusdescription          object        \n",
      " 10  currencyname               object        \n",
      " 11  balance                    float64       \n",
      " 12  originalamount             float64       \n",
      " 13  interest                   float64       \n",
      " 14  pendingcharges             float64       \n",
      " 15  netamount                  float64       \n",
      " 16  lastpayment_date           datetime64[ns]\n",
      " 17  has_courtarchive           bool          \n",
      " 18  courtarchive               object        \n",
      " 19  operationdate              datetime64[ns]\n",
      " 20  reactionid                 object        \n",
      " 21  username                   object        \n",
      " 22  supervisorname             object        \n",
      " 23  exchangerate_sale          float64       \n",
      " 24  secondperiod_crdsrv        object        \n",
      " 25  secondperiodsalary_crdsrv  object        \n",
      " 26  lastsalary                 float64       \n",
      " 27  vehicules_crdsrv           int64         \n",
      " 28  ownerships_crdsrv          int64         \n",
      "dtypes: bool(1), datetime64[ns](3), float64(7), int64(2), object(16)\n",
      "memory usage: 931.1+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar los datos\n",
    "CARTERA = pd.read_parquet(\"C:/Users/gurenab/Documents/Distribuciones/CARTERA.parquet\")\n",
    "CARTERA.info()\n",
    "\n",
    "# Filtros\n",
    "CARTERA = CARTERA[CARTERA['cli_original_code'].notna()]\n",
    "CARTERA = CARTERA[CARTERA['countryname'] == \"COSTA RICA\"]\n",
    "CARTERA = CARTERA[~CARTERA['fullname'].isin([\"VEHICULOS VENDIDOS\", \"CUENTAS CANCELADAS CR\", \"VEHICULOS CAPTURADOS\",\n",
    "                                             \"CUENTAS FALLECIDOS CR\", \"CORPORATIVO INACTIVO\", \"CUENTAS EXCLUIDAS CR\",\n",
    "                                             \"CUENTAS EXCLUIDAS GT\", \"CUENTAS PRESCRITAS LEGALMENTE\"])]\n",
    "\n",
    "# Formato para cálculo disimilitudes\n",
    "CARTERA['secondperiod_crdsrv'] = pd.to_datetime(CARTERA['secondperiod_crdsrv'], errors='coerce')\n",
    "CARTERA['trabaja'] = np.where(CARTERA['secondperiod_crdsrv'] == CARTERA['secondperiod_crdsrv'].max(), \"SI\", \"NO\")\n",
    "CARTERA['trabaja'] = CARTERA['trabaja'].fillna(\"NO\")\n",
    "CARTERA['trabaja'] = CARTERA['trabaja'].astype('category')\n",
    "\n",
    "# Crear la columna 'expediente_valido'\n",
    "valid_exp = [\"90-\", \"91-\", \"92-\", \"93-\", \"94-\", \"95-\", \"96-\", \"97-\", \"98-\", \"99-\", \"00-\", \"01-\", \"02-\", \"03-\", \n",
    "             \"04-\", \"05-\", \"06-\", \"07-\", \"08-\", \"09-\", \"10-\", \"11-\", \"12-\", \"13-\", \"14-\", \"15-\", \"16-\", \"17-\", \n",
    "             \"18-\", \"19-\", \"20-\", \"21-\", \"22-\", \"23-\", \"24-\", \"25-\", \"26-\", \"27-\", \"28-\", \"29-\"]\n",
    "\n",
    "CARTERA['expediente_valido'] = np.where(CARTERA['courtarchive'].str.startswith(tuple(valid_exp)), \"SI\", \"NO\")\n",
    "CARTERA['expediente_valido'] = CARTERA['expediente_valido'].astype('category')\n",
    "\n",
    "# Crear la columna 'rango_salario'\n",
    "CARTERA['rango_salario'] = pd.cut(CARTERA['lastsalary'], \n",
    "                                  bins=[-np.inf, 0, 300000, 450000, 600000, 1000000, np.inf], \n",
    "                                  labels=[\"NA_o_Negativo\", \"0_a_299_999\", \"300_000_a_449_999\", \n",
    "                                          \"450_000_a_599_999\", \"600_000_a_999_999\", \"1_000_000_y_mas\"],\n",
    "                                  ordered=True)\n",
    "\n",
    "# Crear la columna 'rango_deuda'\n",
    "CARTERA['rango_deuda'] = pd.cut(CARTERA['balance'], \n",
    "                                bins=[-np.inf, 0, 250000, 500000, 750000, 1000000, 1500000, 3000000, np.inf], \n",
    "                                labels=[\"NA o Negativo\", \"0_a_249_999\", \"250_000_a_499_999\", \n",
    "                                        \"500_000_a_749_999\", \"750_000_a_999_999\", \"1_000_000_a_1_499_999\", \n",
    "                                        \"1_500_000_a_2_999_999\", \"3_000_000_y_mas\"],\n",
    "                                ordered=True)\n",
    "\n",
    "# Crear la columna 'cantidad_vehiculos'\n",
    "CARTERA['cantidad_vehiculos'] = pd.cut(CARTERA['vehicules_crdsrv'].fillna(0), \n",
    "                                       bins=[-np.inf, 0, 1, 2, np.inf], \n",
    "                                       labels=[\"0\", \"1\", \"2\", \"3_o_mas\"],\n",
    "                                       ordered=True)\n",
    "\n",
    "# Crear la columna 'cantidad_propiedades'\n",
    "CARTERA['cantidad_propiedades'] = pd.cut(CARTERA['ownerships_crdsrv'].fillna(0), \n",
    "                                         bins=[-np.inf, 0, 1, 2, np.inf], \n",
    "                                         labels=[\"0\", \"1\", \"2\", \"3_o_mas\"],\n",
    "                                         ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CARTERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de variables para entrenar el modelo\n",
    "base_dbscan = CARTERA[CARTERA['statusdescription'] == \"Activa\"]\n",
    "\n",
    "# Selección de columnas\n",
    "base_dbscan = base_dbscan[['operationnumber', 'trabaja', 'expediente_valido', \n",
    "                           'rango_salario', 'rango_deuda', \n",
    "                           'cantidad_vehiculos', 'cantidad_propiedades']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19814 entries, 11967 to 4309032\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype   \n",
      "---  ------                --------------  -----   \n",
      " 0   operationnumber       19814 non-null  object  \n",
      " 1   trabaja               19814 non-null  category\n",
      " 2   expediente_valido     19814 non-null  category\n",
      " 3   rango_salario         18632 non-null  category\n",
      " 4   rango_deuda           19814 non-null  category\n",
      " 5   cantidad_vehiculos    19814 non-null  category\n",
      " 6   cantidad_propiedades  19814 non-null  category\n",
      "dtypes: category(6), object(1)\n",
      "memory usage: 426.9+ KB\n"
     ]
    }
   ],
   "source": [
    "base_dbscan.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor eps: 0.1\n",
      "Mejor min_samples: 5\n",
      "Mejor silueta: 0.6494431495666504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Definir rangos de eps y min_samples para buscar\n",
    "eps_values = np.arange(0.1, 0.5, 0.05)\n",
    "min_samples_values = range(5, 20, 2)\n",
    "\n",
    "mejor_eps = None\n",
    "mejor_min_samples = None\n",
    "mejor_silueta = -1\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=\"precomputed\")\n",
    "        agrupacion = dbscan.fit(gower_dist)\n",
    "        labels = agrupacion.labels_\n",
    "        \n",
    "        # Excluir el ruido (label -1) al calcular la silueta\n",
    "        if len(set(labels)) > 1:\n",
    "            silueta = silhouette_score(gower_dist, labels, metric=\"precomputed\")\n",
    "            if silueta > mejor_silueta:\n",
    "                mejor_eps = eps\n",
    "                mejor_min_samples = min_samples\n",
    "                mejor_silueta = silueta\n",
    "\n",
    "print(f\"Mejor eps: {mejor_eps}\")\n",
    "print(f\"Mejor min_samples: {mejor_min_samples}\")\n",
    "print(f\"Mejor silueta: {mejor_silueta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de cálculo de distancias: 0:01:58.137038\n",
      "Mejor eps: None\n",
      "Mejor min_samples: None\n",
      "Mejor silueta: -1\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'eps' parameter of DBSCAN must be a float in the range (0.0, inf). Got None instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m inicio \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     53\u001b[0m dbscan \u001b[38;5;241m=\u001b[39m DBSCAN(eps\u001b[38;5;241m=\u001b[39mmejor_eps, min_samples\u001b[38;5;241m=\u001b[39mmejor_min_samples, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m agrupacion \u001b[38;5;241m=\u001b[39m \u001b[43mdbscan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgower_dist\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m fin \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     56\u001b[0m tiempo_ajuste_modelo_activas \u001b[38;5;241m=\u001b[39m fin \u001b[38;5;241m-\u001b[39m inicio\n",
      "File \u001b[1;32mc:\\Users\\gurenab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1466\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1462\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1463\u001b[0m )\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1466\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gurenab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gurenab\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'eps' parameter of DBSCAN must be a float in the range (0.0, inf). Got None instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances, silhouette_samples\n",
    "from sklearn.cluster import DBSCAN\n",
    "from gower import gower_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "# Convertir las columnas categóricas a strings\n",
    "base_dbscan = base_dbscan.copy()\n",
    "for col in base_dbscan.select_dtypes(['category']).columns:\n",
    "    base_dbscan[col] = base_dbscan[col].astype(str)\n",
    "\n",
    "# Cálculo de disimilitudes utilizando la métrica de Gower\n",
    "inicio = datetime.now()\n",
    "gower_dist = gower_matrix(base_dbscan.iloc[:, 1:])\n",
    "fin = datetime.now()\n",
    "tiempo_distancias_activas = fin - inicio\n",
    "print(\"Tiempo de cálculo de distancias:\", tiempo_distancias_activas)\n",
    "\n",
    "######\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Definir rangos de eps y min_samples para buscar\n",
    "eps_values = np.arange(0.5, 1.5, 0.1)\n",
    "min_samples_values = range(5, 20, 2)\n",
    "\n",
    "mejor_eps = None\n",
    "mejor_min_samples = None\n",
    "mejor_silueta = -1\n",
    "\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=\"precomputed\")\n",
    "        agrupacion = dbscan.fit(gower_dist)\n",
    "        labels = agrupacion.labels_\n",
    "        \n",
    "        # Excluir el ruido (label -1) al calcular la silueta\n",
    "        if len(set(labels)) > 1:\n",
    "            silueta = silhouette_score(gower_dist, labels, metric=\"precomputed\")\n",
    "            if silueta > mejor_silueta:\n",
    "                mejor_eps = eps\n",
    "                mejor_min_samples = min_samples\n",
    "                mejor_silueta = silueta\n",
    "\n",
    "print(f\"Mejor eps: {mejor_eps}\")\n",
    "print(f\"Mejor min_samples: {mejor_min_samples}\")\n",
    "print(f\"Mejor silueta: {mejor_silueta}\")\n",
    "\n",
    "######\n",
    "\n",
    "# Ajuste del modelo DBSCAN\n",
    "inicio = datetime.now()\n",
    "dbscan = DBSCAN(eps=mejor_eps, min_samples=mejor_min_samples, metric=\"precomputed\")\n",
    "agrupacion = dbscan.fit(gower_dist)\n",
    "fin = datetime.now()\n",
    "tiempo_ajuste_modelo_activas = fin - inicio\n",
    "print(\"Tiempo de ajuste del modelo:\", tiempo_ajuste_modelo_activas)\n",
    "\n",
    "# Asignar clusters al DataFrame\n",
    "base_dbscan['coord'] = np.arange(1, len(base_dbscan) + 1)\n",
    "base_dbscan['perfil'] = agrupacion.labels_\n",
    "\n",
    "# Verifica cuántos clusters se formaron\n",
    "num_clusters = len(set(base_dbscan['perfil']) - {-1})  # -1 es el ruido, no es un cluster real\n",
    "print(f\"Número de clusters formados: {num_clusters}\")\n",
    "\n",
    "# # Caracterización de las agrupaciones\n",
    "# base_dbscan['perfil_descripcion'] = np.select(\n",
    "#     [\n",
    "#         base_dbscan['perfil'] == -1,\n",
    "#         (base_dbscan['trabaja'] == \"SI\") & (base_dbscan['expediente_valido'] == \"SI\"),\n",
    "#         (base_dbscan['trabaja'] == \"NO\") & (base_dbscan['expediente_valido'] == \"SI\"),\n",
    "#         (base_dbscan['trabaja'] == \"SI\") & (base_dbscan['expediente_valido'] == \"NO\"),\n",
    "#         (base_dbscan['trabaja'] == \"NO\") & (base_dbscan['expediente_valido'] == \"NO\"),\n",
    "#     ],\n",
    "#     [\n",
    "#         \"Ruido\",\n",
    "#         \"T.si|E.si\",\n",
    "#         \"T.no|E.si\",\n",
    "#         \"T.si|E.no\",\n",
    "#         \"T.no|E.no\"\n",
    "#     ],\n",
    "#     default=\"Otro\"\n",
    "# )\n",
    "# base_dbscan['perfil_descripcion'] = pd.Categorical(base_dbscan['perfil_descripcion'], \n",
    "#                                                    categories=[\"Ruido\", \"T.no|E.no\", \"T.si|E.no\", \"T.no|E.si\", \"T.si|E.si\"], \n",
    "#                                                    ordered=True)\n",
    "\n",
    "# # Crear particiones por clasificación\n",
    "# base_dbscan_ruido = base_dbscan[base_dbscan['perfil_descripcion'] == \"Ruido\"]\n",
    "# base_dbscan_ts_es = base_dbscan[base_dbscan['perfil_descripcion'] == \"T.si|E.si\"]\n",
    "# base_dbscan_tn_es = base_dbscan[base_dbscan['perfil_descripcion'] == \"T.no|E.si\"]\n",
    "# base_dbscan_ts_en = base_dbscan[base_dbscan['perfil_descripcion'] == \"T.si|E.no\"]\n",
    "# base_dbscan_tn_en = base_dbscan[base_dbscan['perfil_descripcion'] == \"T.no|E.no\"]\n",
    "\n",
    "# # Asignación de prioridades (requiere definición de la función 'prioridades')\n",
    "# base_dbscan_ruido['prioridad'] = \"Sin prioridad\"\n",
    "# base_dbscan_ts_es = prioridades(base_dbscan_ts_es)\n",
    "# base_dbscan_tn_es = prioridades(base_dbscan_tn_es)\n",
    "# base_dbscan_ts_en = prioridades(base_dbscan_ts_en)\n",
    "# base_dbscan_tn_en = prioridades(base_dbscan_tn_en)\n",
    "\n",
    "# base_dbscan_prioridades = pd.concat([base_dbscan_ruido, base_dbscan_ts_es, base_dbscan_tn_es, \n",
    "#                                      base_dbscan_ts_en, base_dbscan_tn_en])\n",
    "\n",
    "# # Asignación de prioridades conjuntas (requiere definición de la función 'prioridades_conjuntas')\n",
    "# base_dbscan_prioridades = prioridades_conjuntas(base_dbscan_prioridades)\n",
    "\n",
    "# # Calidad de los agrupamientos (cálculo de siluetas)\n",
    "# inicio = datetime.now()\n",
    "\n",
    "# # Verificación de las columnas antes de calcular las siluetas\n",
    "# print(\"Columnas del DataFrame antes de calcular las siluetas:\", base_dbscan_prioridades.columns)\n",
    "\n",
    "# if 'perfil' in base_dbscan_prioridades.columns:\n",
    "#     # Verifica el rango de columnas que estás pasando a la función\n",
    "#     print(\"Número de columnas en el DataFrame:\", base_dbscan_prioridades.shape[1])\n",
    "#     columnas_esperadas = base_dbscan_prioridades.iloc[:, 1:7].columns\n",
    "#     print(\"Columnas esperadas para el cálculo de siluetas:\", columnas_esperadas)\n",
    "    \n",
    "#     # Verifica si el índice del DataFrame es adecuado\n",
    "#     print(\"Índices del DataFrame:\", base_dbscan_prioridades.index)\n",
    "\n",
    "#     # Llama a la función silueta_promedio\n",
    "#     def silueta_promedio(X, labels):\n",
    "#         \"\"\"Calcula la silueta promedio para cada etiqueta\"\"\"\n",
    "#         try:\n",
    "#             sil_samples = silhouette_samples(X, labels, metric='precomputed')\n",
    "#             return pd.DataFrame({'siluetas_modelo': sil_samples}, index=labels.index)\n",
    "#         except Exception as e:\n",
    "#             print(\"Error en el cálculo de la silueta:\", e)\n",
    "#             return None\n",
    "\n",
    "#     siluetas_modelo = silueta_promedio(base_dbscan_prioridades.iloc[:, 1:7], base_dbscan_prioridades['perfil'])\n",
    "# else:\n",
    "#     print(\"La columna 'perfil' no está en el DataFrame\")\n",
    "#     siluetas_modelo = None\n",
    "\n",
    "# fin = datetime.now()\n",
    "# tiempo_siluetas_originales = fin - inicio\n",
    "# print(\"Tiempo de cálculo de siluetas:\", tiempo_siluetas_originales)\n",
    "\n",
    "# # Asignar siluetas al DataFrame (si no hubo problemas en el cálculo)\n",
    "# if siluetas_modelo is not None:\n",
    "#     base_dbscan_prioridades = pd.concat([base_dbscan_prioridades, siluetas_modelo], axis=1)\n",
    "\n",
    "# # Calcular la silueta promedio por perfil, solo si las siluetas fueron calculadas\n",
    "# if 'siluetas_modelo' in base_dbscan_prioridades.columns:\n",
    "#     silueta_modelo = base_dbscan_prioridades.groupby('perfil').agg(silueta_por_perfil=('siluetas_modelo', 'mean'))\n",
    "#     print(\"Silueta promedio por perfil:\")\n",
    "#     print(silueta_modelo)\n",
    "\n",
    "#     # Silueta promedio de los perfiles, excluyendo el ruido (perfil -1)\n",
    "#     mean_silueta = silueta_modelo.loc[silueta_modelo.index != -1, 'silueta_por_perfil'].mean()\n",
    "#     print(\"Silueta promedio excluyendo el ruido:\", mean_silueta)\n",
    "# else:\n",
    "#     print(\"No se calcularon las siluetas, por lo tanto no se puede calcular la silueta promedio por perfil.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Frecuencia  Proporción\n",
      "perfil_descripcion                        \n",
      "Ruido                     4040    0.203896\n",
      "T.no|E.no                 1074    0.054204\n",
      "T.no|E.si                 2975    0.150146\n",
      "T.si|E.no                 2463    0.124306\n",
      "T.si|E.si                 9262    0.467447\n"
     ]
    }
   ],
   "source": [
    "tabla_absoluta = pd.crosstab(index=base_dbscan['perfil_descripcion'], columns='Frecuencia')\n",
    "tabla_relativa = pd.crosstab(index=base_dbscan['perfil_descripcion'], columns='Proporción', normalize='columns')\n",
    "tabla_combinada = pd.concat([tabla_absoluta, tabla_relativa], axis=1)\n",
    "tabla_combinada.columns = ['Frecuencia', 'Proporción']\n",
    "print(tabla_combinada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dbscan_prioridades"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
